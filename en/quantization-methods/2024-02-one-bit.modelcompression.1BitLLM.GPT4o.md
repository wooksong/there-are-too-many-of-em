# OneBit: Towards Extremely Low-bit Large Language Models

- **Published Date**: 2024-02
- **Authors**: Yuzhuang Xu, Xu Han, Zonghan Yang, Shuo Wang, Qingfu Zhu, Zhiyuan Liu, Weidong Liu, Wanxiang Che
- **Link**: https://arxiv.org/abs/2402.11295
- **Summary Date**: 2025-03-09
- **Summarized by**: @wooksong
- **Tool Used**: GPT-4o
- **Tags**: #OneBit, #LLMQuantization, #1BitLLM, #ModelCompression, #KnowledgeDistillation

## Introduction: The Challenge of Extreme Quantization

Large Language Models (LLMs) have revolutionized natural language processing, but their deployment remains constrained by computational intensity and memory demands. Even moderately-sized models, such as LLaMA-13B, require approximately **26GB** in FP16 format, making them difficult to run on low-end hardware. The authors argue:

> "Such overheads make deploying LLMs difficult beyond mid-to-high-end GPUs like the A100, let alone on mobile devices."

To address this, **quantization**â€”reducing model precision to fewer bitsâ€”has become a key optimization strategy. Existing methods such as **GPTQ**, **AWQ**, and **SpQR** effectively compress LLMs to 4-bit representations while preserving performance. However, they fail at extreme low-bit quantization (e.g., 1-bit), suffering severe degradation. The core motivation behind *OneBit* is:

> "This paper boldly quantizes the weight matrices of LLMs to 1-bit, paving the way for the extremely low bit-width deployment of LLMs."

## The Fundamental Issue: Why is 1-Bit Quantization Hard?

The central challenge of extreme quantization is the **precision loss** of weight matrices. At 1-bit, weights are reduced to binary values **(+1, -1)**, eliminating intermediate numerical representations. The authors note:

> "The drastic precision loss at extremely low bit-width representation significantly increases loss in linear projection WX, which is the core operator within LLMs."

This loss results in poor generalization, unstable training, and degradation of reasoning capabilities. The paper introduces a new **1-bit model compression framework, OneBit**, which tackles these issues through two key innovations:

1. **A novel 1-bit parameter representation method** using a **Sign-Value-Independent Decomposition (SVID)**
2. **A parameter initialization strategy based on matrix decomposition** to improve convergence.

## The OneBit Solution: Redefining 1-Bit Representation

Unlike prior 1-bit approaches that apply a naive **Round-to-Nearest (RTN) quantization**, OneBit introduces a structured **Sign-Value-Independent Decomposition (SVID)**. The key insight:

> "Each original high-bit weight matrix is represented as one sign matrix (Â±1) and two value vectors."

Mathematically, the weight matrix $W$ is decomposed as:

$W â‰ˆ W_{sign} âŠ™ (a b^T)$

where:

- $W_{sign}$ is the **binary sign matrix** (Â±1).
- $a$ and $b$ are **low-rank floating-point value vectors**, restoring some precision.

Unlike previous 1-bit quantization methods that struggle with catastrophic precision loss, OneBit retains enough numerical flexibility to maintain **high-rank approximation** of original weights while achieving extreme compression.

## Training with OneBit: Overcoming the Bottleneck

To further improve training stability, OneBit incorporates **quantization-aware knowledge distillation (KD)**, transferring knowledge from the original full-precision model. The authors emphasize:

> "Our method performs well at the W1A16 (1-bit weight and 16-bit activation) quantization level."

The training objective consists of two key losses:

1. **Logit-based distillation loss (cross-entropy):**
   $L_{CE} = - \frac{1}{n_s} \sum_{i=1}^{n_s} \sum_c P_T(c | o_i) \log P_S(c | o_i)$

2. **Hidden state mean-squared error (MSE) loss:**
   $L_{MSE} = \sum_{i=1}^{n_s} \sum_{j=1}^{n_l} || \frac{q_T}{||q_T||_2} - \frac{q_S}{||q_S||_2} ||_2^2$

where:

- $T$ and $S$ refer to the teacher and student models, respectively.

- $q_T$ and $q_S$ denote the hidden states of the models.

The final knowledge distillation objective balances these two components:

$L_{KD} = L_{CE} + Î± L_{MSE}$

where $Î±$ is a hyperparameter controlling the trade-off between classification and hidden-state alignment.

## Experimental Validation: Does OneBit Work?

To evaluate OneBit, the authors conduct **extensive experiments** across multiple LLM architectures, including **LLaMA**, **LLaMA2**, and **OPT**, ranging from **1.3B to 13B parameters**. Key benchmarks include:

- **Perplexity (lower is better)**
- **Zero-shot accuracy on tasks like Winogrande, HellaSwag, and ARC**
- **Comparison with 2-bit baselines (GPTQ, LLM-QAT, and OmniQuant)**

### Key Results

| Model | Method | Perplexity (Wiki2) | Perplexity (C4) | Zero-shot Avg. |
|--------|--------|------------------|---------------|---------------|
| LLaMA-7B | **FP16** | **5.68** | **7.08** | **64.06** |
| LLaMA-7B | GPTQ | 1900+ | 780+ | 37.02 |
| LLaMA-7B | OmniQuant | 15.34 | 26.21 | 48.17 |
| LLaMA-7B | **OneBit (W1A16)** | **10.19** | **11.40** | **51.33** |

Despite being **only 1-bit**, OneBit **outperforms existing 2-bit methods**, demonstrating strong **generalization** across tasks. The authors highlight:

> "Our method approaches the performance of FP16 more closely as the model size increases."

For instance, on **LLaMA-13B**, OneBit achieves **62.9% accuracy**, reducing the gap with FP16 significantly.

## Implications: The Future of 1-Bit LLMs

The success of OneBit suggests that **extreme low-bit quantization is viable**, contradicting prior assumptions that 1-bit models are impractical. The authors highlight three major advantages:

1. **Massive memory reduction:**
   - LLaMA-13B compresses from **26GB to 2.2GB**, a **91.5% reduction**.
   - Enables LLM deployment on **consumer-grade GPUs, mobile devices, and edge computing**.

2. **Improved training efficiency:**
   - **OneBit-trained models converge faster**, thanks to **SVID-based initialization**.
   - Training is **2% of the full LLaMA-7B pretraining cost**.

3. **Scalability and Stability:**
   - **Avoids catastrophic precision loss** seen in prior 1-bit methods.
   - **More robust** than BitNet and other binary LLM architectures.

## Conclusion: Is OneBit the Future?

The **key takeaway** from this study is that **intelligence does not require high-bit precision**. The authors conclude:

> "Our findings establish OneBit as a promising approach for extreme low-bit LLM quantization, achieving an optimal trade-off between compression and performance."

The success of OneBit raises fundamental questions about **whether high-bit models are truly necessary** for effective reasoning. Future research should explore:

- Scaling **OneBit to larger architectures (30B+)**
- Optimizing **1-bit inference on custom hardware**
- **Multi-modal applications** of extreme quantization.

### Final Thought: The Feynman Perspective

Applying the **Feynman Technique**, we distill the key insight:

ðŸ’¡ **Reducing bit-width does not destroy intelligenceâ€”it refines it.**

The conventional wisdom that high precision is necessary for deep learning may soon be obsolete. **OneBit** suggests that **1-bit LLMs are not just feasible, but practical**, potentially **democratizing AI access** for billions of users. ðŸš€

---

### References

- Xu, Han, Yang, et al. *OneBit: Towards Extremely Low-bit Large Language Models*. NeurIPS 2024.
- Touvron et al. *LLaMA: Open and Efficient Foundation Models*. ArXiv, 2023.
- Vaswani et al. *Attention is All You Need*. NeurIPS, 2017.
