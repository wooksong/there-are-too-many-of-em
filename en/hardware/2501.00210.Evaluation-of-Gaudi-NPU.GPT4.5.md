# Debunking the CUDA Myth Towards GPU-based AI Systems: Evaluation of Intel's Gaudi NPU for AI Model Serving

---

- **Published Date**: 2024-12
- **Authors**: "Yunjae Lee", "Juntaek Lim", "Jehyeon Bang", "Eunyeong Cho", "Huijong Jeong", "Taesu Kim", "Hyungjun Kim", "Joonhyung Lee", "Jinseop Im", "Ranggi Hwang", "Se Jung Kwon", "Dongsoo Lee", "Minsoo Rhu"
- **Affiliation**: "KAIST", "NAVER Cloud", "SqueezeBits"
- **Link**: https://arxiv.org/abs/2402.11295
- **Summary Date**: 2025-03-26
- **Summarized by**: @wooksong
- **Tool Used**: GPT-4.5

---

## Abstract

With the pervasive adoption of AI, NVIDIA GPUs have become the de facto standard for AI system design. This paper provides a comprehensive evaluation of Intel’s Gaudi NPUs as an alternative to NVIDIA GPUs for AI model serving, demonstrating that Gaudi-2 exhibits competitive performance in primitive AI compute, memory, and communication tasks and excels in certain end-to-end AI workloads. Although Gaudi-2 showcases promising energy efficiency, particularly in large language model (LLM) applications, limitations persist due to the relative immaturity of its software ecosystem compared to NVIDIA's CUDA-based framework. The authors conclude:

> "Gaudi NPUs could challenge NVIDIA GPU’s dominance in the AI server market, though further improvements are necessary to fully compete with NVIDIA’s robust software ecosystem."

## Introduction

Historically, NVIDIA GPUs, leveraging CUDA, have overtaken Intel CPUs in AI computing. Despite NPUs (e.g., Google's TPU, Amazon's Inferentia) representing potential GPU alternatives, their usage remains limited due to high development costs. Gaudi NPUs differ by offering native programmability via TPC-C and commercial availability, positioning them uniquely against NVIDIA GPUs.

A notable historical analogy motivates this study:

> "After applying optimizations appropriate for both CPUs and GPUs the performance gap between NVIDIA GTX280 and Intel Core i7 960 narrows to only 2.5×." (ISCA, 2010)

## Intel Gaudi Hardware Architecture

### Compute

Gaudi NPU architecture integrates Matrix Multiplication Engines (MME) and programmable Tensor Processing Cores (TPC):

- **MME**:
  - Two configurable 256×256 MAC units per Gaudi-2
  - Peak BF16 performance: 432 TFLOPS

- **TPC**:
  - 24 fully programmable VLIW vector cores
  - 2048-bit wide SIMD, 11 TFLOPS BF16

Gaudi-2 provides higher aggregate compute throughput and bandwidth compared to A100:

| Spec                  | NVIDIA A100 | Intel Gaudi-2 | Ratio |
|-----------------------|-------------|---------------|-------|
| TFLOPS (Matrix BF16)  | 312         | 432           | 1.4×  |
| TFLOPS (Vector BF16)  | 39          | 11            | 0.3×  |
| HBM Bandwidth         | 2 TB/s      | 2.46 TB/s     | 1.2×  |
| Power Consumption     | 400 W       | 600 W         | 1.5×  |

### Memory and Communication

Gaudi-2 features 96 GB HBM2E memory at 2.46 TB/s, higher than A100’s 2.0 TB/s. Communication relies on RoCE links with linearly scaling bandwidth, unlike NVIDIA’s NVSwitch, which offers constant bandwidth.

## Intel Gaudi Software Architecture

The Gaudi software includes:

- **TPC-C Programming Model**:
  - Single Program Multiple Data (SPMD)
  - Best practices: Aligning data access granularity to 256-byte boundaries, loop unrolling by a factor of 4 for maximizing parallelism

- **Graph Compiler**:
  - Automatically converts PyTorch/TensorFlow models into optimized Gaudi-specific executables
  - Performs kernel fusion and pipeline optimization but lacks user-controlled optimization pass customization:

    > "Users cannot modify the behavior of the graph compiler nor dictate when a particular optimization should be activated."

## Characterizing Gaudi NPU Performance

### Primitive Compute Operations

Gaudi-2 significantly outperforms A100 in GEMM operations due to dynamically reconfigurable MMEs:

> "Gaudi-2 achieves 429 TFLOPS when \( M=K=N=8192 \), reaching 99.3% of its peak throughput."

However, Gaudi-2 underperforms in vector operations due to lower SIMD compute throughput compared to A100.

### Primitive Memory Operations

Gaudi-2 is competitive with streaming memory access but struggles with random accesses smaller than its 256-byte memory granularity:

> "Gaudi-2 achieves only an average 15% memory throughput... compared to A100’s average 36% utilization."

### Primitive Communication Operations

Gaudi-2 demonstrates high collective communication performance only when all eight devices communicate. Performance diminishes rapidly without an NVSwitch-like all-to-all network:

> "Gaudi-2 experiences an almost linear decline in bus bandwidth utilization... due to the absence of an all-to-all switch like NVSwitch."

## End-to-End Application-level Analysis

Evaluations of RecSys and LLM workloads provided clear comparative outcomes:

- **RecSys**:
  - Gaudi-2 performs competitively with large embedding vectors (≥256 bytes) but significantly lags for smaller vectors:

    > "Overall, Gaudi-2 experiences an average performance degradation of 22% (RM1) and 18% (RM2) compared to A100."

- **LLM**:
  - Single-device execution:

    > "Gaudi-2 consistently outperforms A100, achieving an average speedup of 1.47× (max 1.70×)."

  - Multi-device setups:

    > "Average speedups of 1.29×, 1.32×, and 1.35× for 2, 4, and 8 devices respectively."

Gaudi-2 achieves superior energy efficiency compared to A100 in LLM scenarios, averaging a 64% improvement for single-device and 74% for multi-device setups.

## Characterizing Gaudi NPU Programmability

### Low-level Optimization (TPC-C): DLRM Case Study

Custom-developed "BatchedTable" embedding operator provided substantial improvement:

> "Our Gaudi-2 BatchedTable achieves an average memory bandwidth utilization of 34.2%, representing a 1.52× improvement over SingleTable."

However, Gaudi-2 remains disadvantaged for small vector sizes compared to A100.

### High-level Optimization (PyTorch): vLLM Case Study

Optimized vLLM implementation for Gaudi significantly improved performance:

> "vLLM_opt achieves 7.4× improvement in PagedAttention throughput over vLLM_base."

Nonetheless, direct GPU comparisons remain challenging:

> "vLLM_opt still falls short of A100, achieving an average of 45% of A100’s PagedAttention throughput."

## Discussion and Future Work

Programmability constraints primarily relate to limited low-level MME API access:

> "Gaudi’s reliance on Intel’s proprietary graph compiler... creates challenges for implementing low-level optimizations such as kernel fusion."

The authors suggest evaluating Gaudi's potential for training workloads and comparing against other NPUs for future research.

## Related Work

This study uniquely positions itself by comprehensively evaluating Gaudi NPU’s performance, energy efficiency, and programmability, contrasting prior work:

> "the first and most comprehensive characterization of Gaudi NPUs across multiple dimensions, using microbenchmarking, end-to-end energy analysis, and programming case studies."

## Conclusion

Gaudi NPUs demonstrate significant potential as GPU competitors if software stack improvements are realized:

> "As long as Intel properly supports AI frameworks with performance-optimized backend libraries, the CUDA programming system itself might not be as formidable a moat."

## Representative Experimental Results

- **GEMM throughput**: 99.3% utilization (429 TFLOPS)
- **Vector memory**: Only 15% utilization for <256-byte operations
- **LLM serving**: Up to 1.70× speedup over A100

## Mathematical Clarifications

- **Operational intensity** (critical for roofline analysis):

\[
\text{Operational intensity} = \frac{\text{Total operations}}{\text{Memory access size (bytes)}}
\]
