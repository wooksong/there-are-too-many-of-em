# Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster

- **Published Date**: 2023-04
- **Authors**: Cerebras
- **Link**: https://arxiv.org/abs/2304.03208
- **Summary Date**: 2025-03-09
- **Summarized by**: @wooksong
- **Tool Used**: NotebookLM
- **Tags**: #LLM #computeoptimal #efficienttraining #scalinglaws #mup

## 개요

본 문서는 Cerebras Systems에서 개발한 Cerebras-GPT라는 새로운 개방형 연산 최적화 언어 모델 제품군을 소개합니다. 1억 1천만에서 130억 파라미터까지 확장 가능한 이 모델들은 효율적인 사전 훈련 및 스케일링, 공개 데이터셋 및 도구의 최근 연구 발전을 통합하여 개발되었습니다. Cerebras-GPT 모델은 DeepMind의 Chinchilla 스케일링 규칙을 따라 Eleuther Pile 데이터셋에서 훈련되었으며, 이는 주어진 연산 예산에서 최고 정확도를 달성하는 데 목적을 둡니다. 저자들은 Cerebras-GPT 모델이 사전 훈련 및 다운스트림 목표 모두에서 최첨단 훈련 효율성을 보임을 보여주며, 예측 가능한 파워-로우 스케일링을 특징짓습니다. 또한, Maximal Update Parameterization (µP)이 대규모 모델 스케일링, 정확도 향상 및 하이퍼파라미터 예측 가능성을 어떻게 더욱 개선할 수 있는지에 대한 학습 내용을 설명합니다. 본 논문의 주요 기여는 연산 최적 모델 스케일링을 고정된 데이터셋 크기로 훈련된 모델과 비교하는 최초의 개방적이고 재현 가능한 연구라는 점입니다. Cerebras-GPT 모델과 코드는 HuggingFace를 통해 공개됩니다.

## 주요 테마 및 중요 아이디어

### 1. 연산 최적화 언어 모델

- Cerebras-GPT는 DeepMind Chinchilla 스케일링 방법론을 따르는 연산 최적화 언어 모델 제품군입니다.이는 주어진 연산 예산 내에서 최상의 성능을 목표로 모델 크기와 훈련 데이터 크기의 균형을 맞춥니다.

- 저자들은 "모델당 약 20개의 토큰으로 훈련된 모델이 가장 연산 효율적인 사전 훈련을 제공한다는 것을 테스트를 통해 확인했습니다."

### 2. 파레토 프론티어 및 스케일링 법칙

- Cerebras-GPT 모델은 사전 훈련 및 주요 다운스트림 목표 모두에서 연산 최적 파레토 프론티어를 형성합니다.
문서에는 이러한 파레토 프론티어를 특징짓는 스케일링 법칙이 제시되어 향후 모델 및 데이터셋 스케일링 노력의 이점을 예측하는 데 사용할 수 있습니다.

- 구체적인 연산 최적 프론티어 스케일링 법칙은 다음과 같습니다.

  - $L(f) = (f/5.984e22)−0.0737 + 0.5066$ (여기서 f는 연산 FLOPs, L은 손실입니다.)

  - 향후 공개 노력에서는 적절한 모델 크기와 사전 훈련 데이터셋 크기의 균형을 결정할 때 총 연산 예산(사전 훈련 및 예상되는 추론 모두)을 고려해야 한다고 강조합니다.

### 3. 모델 아키텍처 및 훈련

- Cerebras-GPT 모델은 GPT-3와 유사한 자동 회귀 트랜스포머 디코더 아키텍처를 사용합니다. 주요 차이점은 GPT-3가 희소 밴드 어텐션을 사용하는 반면, Cerebras-GPT는 모든 디코더 블록에서 밀집 어텐션을 사용한다는 것입니다.

- 모델은 Pile 데이터셋에서 바이트 쌍 인코딩 및 GPT-2 어휘를 사용하여 훈련되었습니다.

- AdamW 옵티마이저를 사용했으며, 학습률 감쇠 전략과 배치 크기는 모델 크기에 따라 다르게 설정되었습니다. 훈련에는 FP16 혼합 정밀도와 bfloat16 정밀도가 모두 사용되었으며, bfloat16이 더 안정적인 것으로 나타났습니다.

### 4. Maximal Update Parameterization (µP)

- 본 연구에서는 µP가 대규모 모델 스케일링을 개선하고, 정확도를 높이며, 하이퍼파라미터 예측 가능성을 향상시킬 수 있음을 보여줍니다.

- µP를 사용하면 작은 모델에서 얻은 최적의 하이퍼파라미터를 매우 큰 모델에도 동일하게 적용할 수 있어 훈련 노력을 단순화합니다.

- 저자들은 "µP는 폭 관련 훈련 불안정성을 분석적으로 제어하고 작은 모델의 최적 하이퍼파라미터가 매우 큰 모델의 최적 하이퍼파라미터와 동일하도록 허용하는 최초의 포괄적인 방법입니다."

## 실험 결과

- Cerebras-GPT 모델은 Pile 테스트셋 손실 측면에서 GPT-J, GPT-NeoX, Pythia와 같은 공개적으로 사용 가능한 모델과 비교했을 때 최첨단 연산 최적 파레토 프론티어를 달성했습니다.

- 130억 파라미터 모델은 다른 유사한 크기의 공개 모델과 비교하여 대부분의 다운스트림 작업에서 향상된 정확도를 보였습니다.

- µP를 사용하여 구성된 Cerebras-GPT 모델은 표준 파라미터화(SP) 모델에 비해 연산 최적 프론티어 손실이 0.4% 향상되었습니다.

## 훈련 및 추론 FLOPs의 균형

- 본 논문에서는 추론 비용을 고려할 때 모델을 어떻게 사전 훈련해야 하는지 추정할 수 있는 훈련+추론 연산 최적 프론티어를 식별하는 기술을 제시합니다.

- 총 연산 비용은 사전 훈련 FLOPs와 모델의 추론 비용 및 예상되는 추론 토큰 수를 합하여 계산됩니다.

- 저자들은 "만약 모델이 너무 많은 데이터 샘플을 사용하여 사전 훈련 연산 비효율적인 방식으로 훈련된다면, 해당 모델의 훈련 연산 비용이 상각되고 정당화되기 전에 매우 많은 수의 추론에서 사용되어야 할 수 있습니다."

## Cerebras Stack 및 Andromeda AI 슈퍼컴퓨터

- 본 연구는 16개의 Cerebras CS-2 시스템을 포함하는 Cerebras 웨이퍼 스케일 클러스터 "Andromeda"에서 수행되었습니다.

- Cerebras 소프트웨어 플랫폼 (CSoft)의 Weight Streaming 모드는 대규모 모델 훈련을 위한 효율적이고 간단한 접근 방식을 제공하며, 모델 병렬 처리 없이도 GPT-3 175B 파라미터 모델보다 큰 모델을 훈련할 수 있는 능력을 입증했습니다.

- Andromeda는 최대 16개의 CS-2에서 선형 성능 확장을 보여주었습니다.

- 저자들은 "CSoft Weight Streaming은 기존 가속기 방식보다 모델 개발 및 확장이 훨씬 쉽다는 것을 발견했습니다."

## 공개 릴리스
Cerebras-GPT 모델 (1억 1천만에서 130억 파라미터)과 코드는 연구 커뮤니티의 사용 및 재현을 위해 HuggingFace 및 Cerebras Modelzoo를 통해 Apache 2.0 라이선스로 공개됩니다.

## 주요 인용구:

> "Cerebras-GPT 모델은 사전 훈련 및 인기 있는 다운스트림 목표 모두에서 연산 최적 파레토 프론티어를 형성합니다."

> "모델당 약 20개의 토큰으로 훈련된 모델이 가장 연산 효율적인 사전 훈련을 제공한다는 것을 테스트를 통해 확인했습니다."

> "µP는 폭 관련 훈련 불안정성을 분석적으로 제어하고 작은 모델의 최적 하이퍼파라미터가 매우 큰 모델의 최적 하이퍼파라미터와 동일하도록 허용하는 최초의 포괄적인 방법입니다."

> "CSoft Weight Streaming은 기존 가속기 방식보다 모델 개발 및 확장이 훨씬 쉽다는 것을 발견했습니다."

## 결론:

Cerebras-GPT는 연산 효율적인 대규모 언어 모델 훈련을 위한 중요한 진전을 나타냅니다. DeepMind Chinchilla 스케일링 규칙을 따르고 Cerebras의 웨이퍼 스케일 클러스터의 기능을 활용함으로써, 본 연구는 공개적으로 사용 가능하고 재현 가능한 최첨단 모델을 제공합니다. 또한, µP의 적용은 대규모 모델 훈련의 안정성과 효율성을 향상시키는 데 유망한 접근 방식임을 시사합니다. 본 연구에서 제시된 스케일링 법칙 및 분석은 향후 언어 모델 연구 및 개발에 귀중한 통찰력을 제공하며, 훈련 및 추론 비용 간의 균형을 고려하는 총 연산 예산의 중요성을 강조합니다.
