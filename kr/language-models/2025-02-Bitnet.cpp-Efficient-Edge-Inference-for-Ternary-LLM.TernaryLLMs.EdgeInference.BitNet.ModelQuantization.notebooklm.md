# Bitnet.cpp: Efficient Edge Inference for Ternary LLMs

- **Published Date**: 2025-02
- **Authors**: Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, Ruoming Pang
- **Link**: https://arxiv.org/abs/2502.11880
- **Summary Date**: 2025-03-13
- **Summarized by**: @wooksong
- **Tool Used**: NotebookLM
- **Tags**: #TernaryLLMs, #EdgeInference, #BitNet, #ModelQuantization #mpGEMM

## 주요 테마

- **Ternary LLM의 엣지 추론 효율성 향상:** 1-bit LLM인 BitNet b1.58의 등장 이후 ternary LLM에 대한 관심이 높아졌으나, 실제 엣지 환경에서의 효율적인 추론 연구는 미미했습니다. Bitnet.cpp는 이러한 격차를 해소하고자 합니다.

- **mpGEMM (mixed-precision matrix multiplication) 최적화:** Ternary LLM 추론 시간의 대부분을 차지하는 mpGEMM 연산의 효율성을 극대화하기 위해 새로운 mpGEMM 라이브러리를 Bitnet.cpp에 통합했습니다.

- **Sub-2-bits-per-weight (bpw) 효율적이고 무손실 추론:** Bitnet.cpp 라이브러리는 Ternary Lookup Table (TL)과 Int2 with a Scale (I2_S)라는 두 가지 핵심 솔루션을 통해 bpw를 2bit 미만으로 줄이면서도 효율적이고 무손실 추론을 가능하게 합니다.

- **기존 방식의 한계 극복:** 기존 비트 연산 방식의 공간 비효율성과 llama.cpp 등의 방식에서 BitNet b1.58 훈련 방식과의 불일치로 인한 무손실 추론 불가 문제를 해결합니다.

- **폭넓은 적용 가능성 제시:** Ternary LLM뿐만 아니라 low-bit LLM에도 적용 가능한 Element-wise Lookup Table (ELUT) 방안을 제시하고 이론적 및 실험적 가능성을 보여줍니다.

## 주요 아이디어

### Ternary LLM의 특징 및 엣지 추론의 필요성

- Ternary LLM은 가중치를 {-1, 0, 1} 세 가지 값으로 양자화하여 bpw를 약 1.58로 줄여 모델 크기를 크게 압축하고 엣지 기기에서의 효율적인 추론 가능성을 높입니다.
- 데이터 프라이버시 문제로 인해 엣지 기기에서의 LLM 배포 요구가 증가하고 있지만, 제한적인 컴퓨팅 파워와 대역폭으로 인해 어려움이 있습니다.
- BitNet b1.58, TriLM, Llama3-8B-1.58, BitNet a4.8 등의 ternary LLM이 등장하며 그 효과와 적용 가능성이 입증되었으나, 실제 추론에서의 이점 활용은 아직 연구가 부족합니다.

### 기존 mpGEMM 방식의 한계

- Ternary 가중치의 비정수 bpw 특성은 컴퓨터 메모리 접근 방식과 충돌하여 sub-2-bit-per-weight 효율적인 엣지 mpGEMM 설계를 어렵게 합니다.

- llama.cpp의 TQ1_0은 1.69 bits를 사용하여 ternary 가중치를 저장하지만, 2 bits를 사용하는 TQ2_0 및 T-MAC보다 느립니다.

- 기존 mpGEMM 구현은 BitNet b1.58 훈련 방식과 완벽하게 일치하지 않아 무손실 추론을 달성하지 못했습니다.

### Bitnet.cpp의 핵심 솔루션

- **Ternary Lookup Table (TL):** 비트 연산 방식의 공간 비효율성을 해결하고 빠른 추론을 가능하게 하는 요소별 LUT 기반 mpGEMM 방식입니다.

- **Int2 with a Scale (I2_S):** BitNet b1.58 훈련 방식을 엄격하게 준수하여 엣지에서의 무손실 추론을 보장하는 요소별 MAD (multiply-then-add) 기반 mpGEMM 방식입니다.

- Bitnet.cpp는 복잡한 비트 수준 조작을 피하고 가중치 요소를 직접 연산하여 공간 효율성을 높이고 성능을 향상시키며 BitNet b1.58에 대한 무손실 추론을 달성합니다.

### TL의 설계 및 구현

- **요소별 LUT 기반 (ELUT) mpGEMM:** 가중치의 모든 가능한 값을 열거하여 LUT를 구성하고 테이블 조회를 통해 빠르게 누적 연산을 수행합니다.

- **요소별 미러 통합 (Element-wise Mirror Consolidation):** LUT 크기를 절반으로 줄여 메모리 효율성을 높입니다. 이를 통해 TL1 (g=2)과 TL2 (g=3, 요소별 미러 통합 적용) 두 가지 디자인을 개발했습니다.

- **부호-비부호 가중치 분할 (Signed-Unsigned Weight Splitting):** 부호와 크기를 분리하여 저장함으로써 메모리 접근 비정렬 문제를 해결하고 SIMD 명령어와의 호환성을 높입니다.

- **1bit 부호 연산 (1bit Sign Operation):** XOR 및 ADD 연산을 통해 1bit로 부호를 효율적으로 처리하고 AVX2 및 NEON 명령어와의 호환성을 보장합니다.

- **블록 적합 가중치 분할 (Block-fitting Weight Splitting):** 가중치 행렬의 크기가 TL2의 계산 블록 크기의 배수가 아닌 경우, 가중치를 두 부분으로 분할하여 각각 TL2와 TL1으로 처리함으로써 블록 불일치 문제를 해결하고 패딩으로 인한 지연 증가를 방지합니다.

### 무손실 엣지 추론

- **팩-언팩 (Pack-and-Unpack) 기술 (TL1_1, TL2_1):** 테이블 조회 결과의 합이 8-bit 정수 범위를 초과할 수 있는 문제를 해결하기 위해 합을 int16으로 유지하고 팩 및 언팩 연산을 사용하여 정보 손실 없이 결과를 얻습니다.

- **I2_S:** BitNet b1.58 훈련 시의 ternary 가중치 및 per-tensor int8 활성화 양자화 설정을 엄격하게 준수하는 요소별 MAD 기반 방식으로 무손실 추론을 달성합니다. TQ2_0과 유사한 성능을 보이며 K 차원이 128의 배수인 경우에도 지원합니다.

## 실험 결과

- Apple M2 Ultra 및 Intel i7-13700H 엣지 기기에서 Bitnet.cpp의 성능을 평가한 결과, 기존 최첨단 방식 대비 최대 6.25배의 속도 향상을 달성했으며, sub-2-bits-per-weight 조건에서 상당한 성능 우위를 보였습니다.

- TL2_0은 Intel i7-13700H에서 T-MAC 대비 최대 2.32배, Apple M2 Ultra에서 최대 1.19배 빠른 속도를 보였습니다. 또한, TQ1_0 대비 Intel i7-13700H에서 최대 1.33배, Apple M2 Ultra에서 최대 1.65배 빠른 속도를 나타냈습니다.

- 품질 평가 결과, TL1_0 및 TL2_0은 Float16과 거의 동일한 perplexity 및 accuracy를 보였으며, I2_S, TL1_1, TL2_1은 모든 작업에서 Float16 대비 무손실 성능을 입증했습니다.

## ELUT 확장 및 분석 (Appendix)

- Element-wise LUT 기반 방식을 ternary LLM뿐만 아니라 low-bit LLM으로 확장하여 이론적 연산 복잡도 및 메모리 접근 복잡도를 분석하고 MAD 기반 및 비트 연산 기반 방식과의 비교를 통해 ELUT의 포괄적인 장점을 제시합니다.

- 낮은 스레드 수 환경에서 MAD 기반 방식 대비 연산 복잡도 이점을 가지며, 비트 연산 방식 대비 더 세밀한 가중치 압축을 가능하게 합니다.

- 다중 스레드 환경에서의 성능 분석을 통해 TL2_0이 메모리 병목 현상에 늦게 도달하고 성능 상한을 높이는 것을 확인했으며, MAD 기반 방식 대비 연산 관련 성능 이점을 입증했습니다.

- 대역폭, 명령어 처리량, 레지스터 길이 등의 하드웨어 제약 사항을 분석하고, 이러한 제약이 없을 경우 ELUT의 잠재적인 성능 향상 가능성을 제시합니다.

## Bitnet.cpp의 한계

- 현재는 ternary LLM의 엣지 추론만을 지원하며, 다양한 기기로의 확장 계획을 가지고 있습니다.

- 적용 가능한 모델 아키텍처 범위가 ternary LLM으로 비교적 좁으며, ELUT이 low-bit 범위를 포괄하도록 확장되었으나 ternary 외 다른 LLM에 대한 실제 지원은 부족합니다.

- 디코딩 단계에서 프리필링 단계로 자원 병목 지점이 이동함에 따라, 프리필링 단계에서의 LLM 가속화에 대한 상세한 논의는 포함되지 않았으며, 향후 최적화 방법을 탐색할 예정입니다.

## 인용구

- "To bridge this gap, we introduce Bitnet.cpp, an in-ference system optimized for BitNet b1.58 and ternary LLMs."

- "The library features two core solutions: Ternary Lookup Table (TL), which addresses spatial inefficiencies of previ-ous bit-wise methods, and Int2 with a Scale (I2_S), which ensures lossless edge inference, both enabling high-speed inference."

- "Our exper-iments show that Bitnet.cpp achieves up to a 6.25x increase in speed over full-precision base-lines and up to 2.32x over low-bit baselines, setting new benchmarks in the field."

- "Additionally, we expand TL to element-wise lookup ta-ble (ELUT) for low-bit LLMs in the appendix, presenting both theoretical and empirical evi-dence of its considerable potential."

- "Despite the burgeoning interest in ternary LLMs, the conversion of their theoretical benefits into practical advantages during inference is still understudied."

- "Our key idea is to avoid intricate bit-level manipulations by directly operating the weight elements when designing mpGEMM, while strictly aligning with BitNet b1.58 training schemes."

- "We have demonstrated that Bitnet.cpp achieves up to 6.25x speedup compared to baselines and pro-vided lossless inference for BitNet b1.58."

- "This paper presents exten-sive work on optimizing edge inference for ternary LLMs from both algorithmic and engineering per-spectives. It offers the research community new insights into handling ternary and non-integer bpw weights, shows the practical advantages of ternary LLMs and presents the industry with innovative so-lutions for deploying fast, lossless LLMs on edge devices."
