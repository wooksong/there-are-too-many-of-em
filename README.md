# There are too many of 'em

> "There are too many of them!" - Bruce Banner, Avengers: Infinity War <br>
>
> Just like Dr. Banner was overwhelmed in Wakanda, we are drowning in papers and technical reports.<br>
> Let's fight back with the power of LLMs and share our knowledge! ðŸ’ª

![infinity-war-there-are-too-many-of-em](https://github.com/user-attachments/assets/175926d7-4f68-46e4-8af8-bad6d6594ce8)

This repository contains summaries of papers and technical reports generated using LLM tools like NotebookLM and ChatGPT. We believe in collaborative learning and knowledge sharing!

## Language Models

| Date | Paper | Tags | Tool | Language |
|------|-------|------|------|----------|
| 2023-04 | [Cerebras-GPT](https://arxiv.org/abs/2304.03208) | #LLM #computeoptimal #efficienttraining #scalinglaws #mup | NotebookLM | EN [KR](kr/language-models/2023-04-Cerebras-GPT.llm.mup.efficient.notebooklm.md) |
| 2024-09 | [Small Language Models](https://arxiv.org/abs/2409.15790) | #SLM #survey #OnDevice #Quantization #Benchmarking | NotebookLM | EN [KR](kr/language-models/2024-09-Small-Language-Models.slm.survey.md) |
| 2025-02 | [Large Language Diffusion Models](https://arxiv.org/abs/2502.09992) | #LLM, #LLaDA, #Diffusion, #Generative | GPT-4o, NotebookLM | [EN](en/language-models/2025-02-large-language-diffusion-models.llm.llada.diffusion.GPT4o.md) [KR](kr/language-models/2025-02-large-language-diffusion-models.llm.llada.diffusion.notebooklm.md) |
| 2025-02 | [Bitnet.cpp: Efficient Edge Inference for Ternary LLMs](https://arxiv.org/abs/2502.11880) |  #TernaryLLMs, #EdgeInference, #BitNet, #ModelQuantization #mpGEMM | GPT-4.5, NotebookLM | [EN](en/language-models/2025-02-Bitnet.cpp-Efficient-Edge-Inference-for-Ternary-LLM.TernaryLLMs.EdgeInference.BitNet.ModelQuantization.GPT4.5.md) [KR](kr/language-models/2025-02-Bitnet.cpp-Efficient-Edge-Inference-for-Ternary-LLM.TernaryLLMs.EdgeInference.BitNet.ModelQuantization.notebooklm.md) |
| 2025-03 | [Phi-4-Mini Technical Report](https://arxiv.org/abs/2503.01743) | #SLM, #MultiModal, #LoRA, #DataCentricLearning, #Reasoning | NotebookLM | EN [KR](kr/language-models/2025-03-phi4-mini-technical-report.slm.multimodal.efficient.notebooklm.md) |
| 2025-03 | [Block Diffusion](https://arxiv.org/abs/2503.09573) | #DiffusionModels, #BlockDiffusion, #AutoRegressiveModels, #LM, #BD3LM | GPT-4.5 | [EN](en/language-models/2025-03-Block-Diffusion.diffusionmodels.autoregressive.lm.GPT4.5.md) KR |

## Quantization Methods

| Date | Paper | Tags | Tool | Language |
|------|-------|------|------|----------|
| 2024-02 | [OneBit](https://arxiv.org/abs/2402.11295) | #OneBit, #LLMQuantization, #1BitLLM, #ModelCompression, #KnowledgeDistillation | GPT-4o | [EN](./en/quantization-methods/2024-02-one-bit.modelcompression.1BitLLM.GPT4o.md) KR |
| 2024-10 | [Mixture of Scales](https://arxiv.org/abs/2406.12311v1) | #LLM_Binarization, #MemoryEfficientAI, #Quantization, #TokenAdaptiveScaling, #ModelCompression | GPT-4o | [EN](./en/quantization-methods/2024-10-mixture-of-scales.modelcompression.binarization.GPT4o.md) KR |
